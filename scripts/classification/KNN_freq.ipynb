{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt, resample\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0418493  0.0506756  0.12915846 ... 1.10669469 4.32195351 3.77053649]\n",
      " [0.09349911 0.16133139 0.24302722 ... 1.70000567 5.09080783 4.41652664]\n",
      " [0.11789428 0.10897665 0.09611266 ... 0.13616219 0.36092414 0.27227442]\n",
      " ...\n",
      " [0.28932336 0.25342643 0.20882229 ... 0.48709229 2.02649375 4.09828131]\n",
      " [0.64274099 0.43413531 0.24110945 ... 0.30086708 0.78178367 1.19009372]\n",
      " [0.23931071 0.27934634 0.26201754 ... 0.57087999 1.96838703 3.53504059]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"D:\\shared_git\\MaestriaThesis\\FeaturesTabs\\pp01_t3.csv\")\n",
    "\n",
    "new_column_names = ['channels', 'ID', 'Type', 'Epoch']\n",
    "df.rename(columns=dict(zip(df.columns[:4], new_column_names)), inplace=True)\n",
    "\n",
    "filtered_class_labels = df[df['channels'] == 5]\n",
    "filtered_class_labels = df[df['Type'].isin([1,2,3,4,5,6,7,8,9,10,11,12])]\n",
    "filtered_class_labels = filtered_class_labels.drop(columns=['ID'])\n",
    "filtered_class_labels = filtered_class_labels.drop(columns=['channels', 'Epoch'])\n",
    "filtered_class_labels = filtered_class_labels.reset_index(drop=True)\n",
    "\n",
    "data_array = filtered_class_labels.iloc[:, 4:].values\n",
    "print(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Air\n",
      "1       Air\n",
      "2       Air\n",
      "3       Air\n",
      "4       Air\n",
      "       ... \n",
      "6995    Car\n",
      "6996    Car\n",
      "6997    Car\n",
      "6998    Car\n",
      "6999    Car\n",
      "Name: Type, Length: 7000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "class_labels = filtered_class_labels.iloc[:,0]\n",
    "mods = class_labels.copy() \n",
    "mod = ['Air', 'Vib', 'Car']\n",
    "mods[np.isin(class_labels,[1, 2, 3, 4])] = mod[0]\n",
    "mods[np.isin(class_labels, [5, 6, 7, 8])] = mod[1]\n",
    "mods[np.isin(class_labels, [9, 10, 11, 12])] = mod[2]\n",
    "print(mods.head(7000)) \n",
    "class_labels = (class_labels - 1) % 4 + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.17%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = data_array\n",
    "y = mods.values\n",
    "\n",
    "y = y.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=13)\n",
    "\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature index: 0, Mutual Information Score: 0.0\n",
      "Feature index: 1, Mutual Information Score: 0.01583827880454125\n",
      "Feature index: 2, Mutual Information Score: 0.0\n",
      "Feature index: 3, Mutual Information Score: 0.0\n",
      "Feature index: 4, Mutual Information Score: 0.010581728343493246\n",
      "Feature index: 5, Mutual Information Score: 0.0\n",
      "Feature index: 6, Mutual Information Score: 0.011824932677851674\n",
      "Feature index: 7, Mutual Information Score: 0.000286994574032029\n",
      "Feature index: 8, Mutual Information Score: 0.0\n",
      "Feature index: 9, Mutual Information Score: 0.0\n",
      "Feature index: 10, Mutual Information Score: 0.0010869260077090992\n",
      "Feature index: 11, Mutual Information Score: 0.0140393192083903\n",
      "Feature index: 12, Mutual Information Score: 0.0\n",
      "Feature index: 13, Mutual Information Score: 0.002557261020339041\n",
      "Feature index: 14, Mutual Information Score: 0.0\n",
      "Feature index: 15, Mutual Information Score: 0.003057897964903855\n",
      "Feature index: 16, Mutual Information Score: 0.0007519003310978434\n",
      "Feature index: 17, Mutual Information Score: 0.003676471501349088\n",
      "Feature index: 18, Mutual Information Score: 0.0\n",
      "Feature index: 19, Mutual Information Score: 0.0055633163164743316\n",
      "Feature index: 20, Mutual Information Score: 0.0030640374927468628\n",
      "Feature index: 21, Mutual Information Score: 0.0\n",
      "Feature index: 22, Mutual Information Score: 0.00939369157008052\n",
      "Feature index: 23, Mutual Information Score: 0.0\n",
      "Feature index: 24, Mutual Information Score: 0.00576830200114431\n",
      "Feature index: 25, Mutual Information Score: 0.008396687394170765\n",
      "Feature index: 26, Mutual Information Score: 0.005462191856063114\n",
      "Feature index: 27, Mutual Information Score: 0.00012427909422241257\n",
      "Feature index: 28, Mutual Information Score: 0.004498226711803444\n",
      "Feature index: 29, Mutual Information Score: 0.0\n",
      "Feature index: 30, Mutual Information Score: 0.002372607567370899\n",
      "Feature index: 31, Mutual Information Score: 0.0\n",
      "Feature index: 32, Mutual Information Score: 0.0\n",
      "Feature index: 33, Mutual Information Score: 0.0049653775057603156\n",
      "Feature index: 34, Mutual Information Score: 0.0006744889503340623\n",
      "Feature index: 35, Mutual Information Score: 0.0027586913111683486\n",
      "Feature index: 36, Mutual Information Score: 0.00914431017452877\n",
      "Feature index: 37, Mutual Information Score: 0.0\n",
      "Feature index: 38, Mutual Information Score: 0.0\n",
      "Feature index: 39, Mutual Information Score: 0.006126886516020669\n",
      "Feature index: 40, Mutual Information Score: 0.005363460259935504\n",
      "Feature index: 41, Mutual Information Score: 0.009969823430719504\n",
      "Feature index: 42, Mutual Information Score: 0.0028463936518874355\n",
      "Feature index: 43, Mutual Information Score: 0.0033698243355626545\n",
      "Feature index: 44, Mutual Information Score: 0.009822640693795126\n",
      "Feature index: 45, Mutual Information Score: 0.0\n",
      "Feature index: 46, Mutual Information Score: 8.727202891201102e-05\n",
      "Feature index: 47, Mutual Information Score: 0.0\n",
      "Feature index: 48, Mutual Information Score: 0.0022593959744572345\n",
      "Feature index: 49, Mutual Information Score: 0.0\n",
      "Feature index: 50, Mutual Information Score: 0.014377973101646901\n",
      "Feature index: 51, Mutual Information Score: 0.0\n",
      "Feature index: 52, Mutual Information Score: 0.007996155741873068\n",
      "Feature index: 53, Mutual Information Score: 0.018513105860410217\n",
      "Feature index: 54, Mutual Information Score: 0.0\n",
      "Feature index: 55, Mutual Information Score: 0.010179584968777355\n",
      "Feature index: 56, Mutual Information Score: 0.011409878450183264\n",
      "Feature index: 57, Mutual Information Score: 0.009198128645927284\n",
      "Feature index: 58, Mutual Information Score: 0.0068595087880285455\n",
      "Feature index: 59, Mutual Information Score: 0.0\n",
      "Feature index: 60, Mutual Information Score: 0.0\n",
      "Feature index: 61, Mutual Information Score: 0.0\n",
      "Feature index: 62, Mutual Information Score: 0.008973648133868917\n",
      "Feature index: 63, Mutual Information Score: 0.0043656214487732825\n",
      "Feature index: 64, Mutual Information Score: 0.013158303751906963\n",
      "Feature index: 65, Mutual Information Score: 0.0021289498452090605\n",
      "Feature index: 66, Mutual Information Score: 0.005928183388831609\n",
      "Feature index: 67, Mutual Information Score: 0.0\n",
      "Feature index: 68, Mutual Information Score: 0.007698069196409474\n",
      "Feature index: 69, Mutual Information Score: 0.00866946089598608\n",
      "Feature index: 70, Mutual Information Score: 0.0069861119529366356\n",
      "Feature index: 71, Mutual Information Score: 0.012005696895935003\n",
      "Feature index: 72, Mutual Information Score: 0.0036966492854064725\n",
      "Feature index: 73, Mutual Information Score: 0.00022757074149337697\n",
      "Feature index: 74, Mutual Information Score: 0.0053918326147752715\n",
      "Feature index: 75, Mutual Information Score: 0.01396857021803255\n",
      "Feature index: 76, Mutual Information Score: 0.007890326218877863\n",
      "Feature index: 77, Mutual Information Score: 0.007243738729100446\n",
      "Feature index: 78, Mutual Information Score: 0.010601015065154318\n",
      "Feature index: 79, Mutual Information Score: 0.010992974022095314\n",
      "Feature index: 80, Mutual Information Score: 0.0\n",
      "Feature index: 81, Mutual Information Score: 0.012589163538232917\n",
      "Feature index: 82, Mutual Information Score: 0.0\n",
      "Feature index: 83, Mutual Information Score: 0.0\n",
      "Feature index: 84, Mutual Information Score: 0.0025852338761960247\n",
      "Feature index: 85, Mutual Information Score: 0.006967942899215984\n",
      "Feature index: 86, Mutual Information Score: 0.0020418583466503826\n",
      "Feature index: 87, Mutual Information Score: 0.0\n",
      "Feature index: 88, Mutual Information Score: 0.005721263635033669\n",
      "Feature index: 89, Mutual Information Score: 0.009130030145751533\n",
      "Feature index: 90, Mutual Information Score: 0.011418879482506306\n",
      "Feature index: 91, Mutual Information Score: 0.003035926496996133\n",
      "Feature index: 92, Mutual Information Score: 4.4481955130848405e-05\n",
      "Feature index: 93, Mutual Information Score: 0.0\n",
      "Feature index: 94, Mutual Information Score: 0.008086156575525472\n",
      "Feature index: 95, Mutual Information Score: 0.00017222408355888064\n",
      "Feature index: 96, Mutual Information Score: 0.0075193120918113365\n",
      "Feature index: 97, Mutual Information Score: 0.0\n",
      "Feature index: 98, Mutual Information Score: 0.0033671012979734627\n",
      "Feature index: 99, Mutual Information Score: 0.0\n",
      "Feature index: 100, Mutual Information Score: 0.0054363256290135276\n",
      "Feature index: 101, Mutual Information Score: 0.0\n",
      "Feature index: 102, Mutual Information Score: 0.0\n",
      "Feature index: 103, Mutual Information Score: 0.0046765004628182005\n",
      "Feature index: 104, Mutual Information Score: 0.012361658736166792\n",
      "Feature index: 105, Mutual Information Score: 0.009587077807033495\n",
      "Feature index: 106, Mutual Information Score: 0.001740747521311814\n",
      "Feature index: 107, Mutual Information Score: 0.0\n",
      "Feature index: 108, Mutual Information Score: 0.0\n",
      "Feature index: 109, Mutual Information Score: 0.0062935309936036354\n",
      "Feature index: 110, Mutual Information Score: 0.013362741925690091\n",
      "Accuracy with selected features: 46.56%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "mutual_info_scores = mutual_info_classif(X_train, y_train)\n",
    "\n",
    "for feature, score in zip(range(X_train.shape[1]), mutual_info_scores):\n",
    "    print(f\"Feature index: {feature}, Mutual Information Score: {score}\")\n",
    "\n",
    "threshold = 0.0001  \n",
    "\n",
    "selected_features = np.where(mutual_info_scores > threshold)[0]\n",
    "\n",
    "X_train_selected = X_train[:, selected_features]\n",
    "X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with selected features: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   4   6   7  10  11  13  15  16  17  19  20  22  24  25  26  27  28\n",
      "  30  33  34  35  36  39  40  41  42  43  44  48  50  52  53  55  56  57\n",
      "  58  62  63  64  65  66  68  69  70  71  72  73  74  75  76  77  78  79\n",
      "  81  84  85  86  88  89  90  91  94  95  96  98 100 103 104 105 106 109\n",
      " 110]\n"
     ]
    }
   ],
   "source": [
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 43.97%\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=13)\n",
    "\n",
    "knn_model.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred = knn_model.predict(X_test_selected)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_neighbors': 13}\n",
      "Best cross-validation accuracy: 44.26%\n",
      "Test set accuracy: 43.97%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_neighbors': list(range(1, 31))}\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
    "\n",
    "y_pred = grid_search.predict(X_test_selected)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test set accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'algorithm': 'auto', 'leaf_size': 20, 'n_neighbors': 20, 'p': 1, 'weights': 'distance'}\n",
      "Best cross-validation accuracy: 46.65%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 31)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [20, 30, 40],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.2f}%\".format(grid_search.best_score_ * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.03%\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=20,p=1,weights='distance',leaf_size=20,algorithm='auto')\n",
    "\n",
    "knn_model.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred = knn_model.predict(X_test_selected)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 44.04%\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=12)\n",
    "\n",
    "knn_model.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred = knn_model.predict(X_test_selected)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[166], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m gbm_model \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Fit the model on the training data\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mgbm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_selected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Predict the labels for the test set\u001b[39;00m\n\u001b[0;32m     10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m gbm_model\u001b[38;5;241m.\u001b[39mpredict(X_test_selected)\n",
      "File \u001b[1;32md:\\Softwares\\Minoconda\\envs\\MA\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:538\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32md:\\Softwares\\Minoconda\\envs\\MA\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:615\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    608\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    609\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    610\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    611\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    612\u001b[0m     )\n\u001b[0;32m    614\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 615\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32md:\\Softwares\\Minoconda\\envs\\MA\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:233\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mis_multi_class:\n\u001b[0;32m    231\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(original_y \u001b[38;5;241m==\u001b[39m k, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m--> 233\u001b[0m residual \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_predictions_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# induce regression tree on residuals\u001b[39;00m\n\u001b[0;32m    238\u001b[0m tree \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(\n\u001b[0;32m    239\u001b[0m     criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion,\n\u001b[0;32m    240\u001b[0m     splitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m     ccp_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mccp_alpha,\n\u001b[0;32m    250\u001b[0m )\n",
      "File \u001b[1;32md:\\Softwares\\Minoconda\\envs\\MA\\Lib\\site-packages\\sklearn\\ensemble\\_gb_losses.py:824\u001b[0m, in \u001b[0;36mMultinomialDeviance.negative_gradient\u001b[1;34m(self, y, raw_predictions, k, **kwargs)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnegative_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, raw_predictions, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    809\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute negative gradient for the ``k``-th class.\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m        The index of the class.\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(\n\u001b[1;32m--> 824\u001b[0m         np\u001b[38;5;241m.\u001b[39mexp(raw_predictions[:, k] \u001b[38;5;241m-\u001b[39m \u001b[43mlogsumexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    825\u001b[0m     )\n",
      "File \u001b[1;32md:\\Softwares\\Minoconda\\envs\\MA\\Lib\\site-packages\\scipy\\special\\_logsumexp.py:111\u001b[0m, in \u001b[0;36mlogsumexp\u001b[1;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[0;32m    109\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(a \u001b[38;5;241m-\u001b[39m a_max)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# suppress warnings about log of zero\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "\n",
    "gbm_model.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred = gbm_model.predict(X_test_selected)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"GBM Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 41.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\Minoconda\\envs\\MA\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the model\n",
    "log_reg_model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "log_reg_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = log_reg_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
